"""
ðŸŽ¬ AI_MODULE: WorkflowOrchestrator
ðŸŽ¬ AI_DESCRIPTION: Orchestratore principale per workflow video studio
ðŸŽ¬ AI_BUSINESS: Gestisce pipeline completa: acquisizione â†’ estrazione â†’ fusione â†’ traduzione â†’ avatar â†’ rendering
ðŸŽ¬ AI_TEACHING: Pipeline orchestration, error recovery, checkpointing, parallel processing

ðŸ”„ ALTERNATIVE_VALUTATE:
- Celery: Scartato per overhead eccessivo per workflow lineare
- Airflow: Scartato per complessitÃ  eccessiva
- Custom scheduler: Scartato per mancanza features

ðŸ’¡ PERCHÃ‰_QUESTA_SOLUZIONE:
- Async/await: Performance native, controllo fine
- State machine: Gestione stati workflow robusta
- Checkpointing: Recovery automatico da errori
- Parallel processing: Ottimizzazione performance

ðŸ“Š METRICHE_SUCCESSO:
- Pipeline completion: >95%
- Error recovery: <30s
- Processing time: <50% tempo originale
- Memory usage: <4GB per video

ðŸ—ï¸ STRUTTURA LEGO:
- INPUT: Video files, configuration, user preferences
- OUTPUT: Processed videos, metadata, analysis results
- DIPENDENZE: OpenCV, MediaPipe, transformers, ffmpeg
- USATO DA: Video Studio API, batch processing

ðŸŽ¯ RAG_METADATA:
- Tags: workflow, orchestration, pipeline, video-processing, ai
- Categoria: Video Studio Core
- Versione: 1.0.0

TRAINING_PATTERNS:
- Success: Complete workflow execution with quality output
- Failure: Partial processing with recovery and retry
- Feedback: Performance metrics and quality scores logged
"""

import asyncio
import json
import logging
import os
import shutil
import sqlite3
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, asdict
import uuid
import traceback

import cv2
import numpy as np
from PIL import Image
import mediapipe as mp
import torch
from transformers import pipeline

logger = logging.getLogger(__name__)

class WorkflowStage(Enum):
    """Stadi del workflow video"""
    INITIALIZED = "initialized"
    ACQUISITION = "acquisition"
    EXTRACTION = "extraction"
    FUSION = "fusion"
    TRANSLATION = "translation"
    AVATAR_CREATION = "avatar_creation"
    RENDERING = "rendering"
    COMPLETED = "completed"
    FAILED = "failed"

class WorkflowStatus(Enum):
    """Status del workflow"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RETRYING = "retrying"

@dataclass
class WorkflowConfig:
    """Configurazione workflow"""
    input_path: str
    output_dir: str
    stages_enabled: List[WorkflowStage]
    quality_preset: str = "high"
    parallel_workers: int = 4
    checkpoint_interval: int = 30  # seconds
    retry_attempts: int = 3
    timeout_minutes: int = 60
    user_preferences: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.user_preferences is None:
            self.user_preferences = {}

@dataclass
class WorkflowState:
    """Stato workflow"""
    id: str
    status: WorkflowStatus
    current_stage: WorkflowStage
    progress: float
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None
    checkpoints: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.checkpoints is None:
            self.checkpoints = {}

@dataclass
class ProcessingResult:
    """Risultato elaborazione"""
    success: bool
    output_path: Optional[str] = None
    metadata: Dict[str, Any] = None
    error_message: Optional[str] = None
    processing_time: Optional[float] = None
    quality_score: Optional[float] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class WorkflowOrchestrator:
    """
    Orchestratore principale per workflow video studio
    
    Gestisce:
    - Pipeline completa video processing
    - Error recovery e checkpointing
    - Parallel processing
    - State management
    - Progress tracking
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inizializza orchestratore
        
        Args:
            config: Configurazione sistema
        """
        self.config = config
        self.workflows_db = config.get('workflows_db', './data/workflows.db')
        self.temp_dir = Path(config.get('temp_dir', './temp'))
        self.output_dir = Path(config.get('output_dir', './output'))
        
        # Crea directories
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup database
        self.setup_database()
        
        # Inizializza moduli
        self.pose_detector = None
        self.translator = None
        self.avatar_creator = None
        
        # Active workflows
        self.active_workflows: Dict[str, WorkflowState] = {}
        
        # Stage processors
        self.stage_processors = {
            WorkflowStage.ACQUISITION: self._process_acquisition,
            WorkflowStage.EXTRACTION: self._process_extraction,
            WorkflowStage.FUSION: self._process_fusion,
            WorkflowStage.TRANSLATION: self._process_translation,
            WorkflowStage.AVATAR_CREATION: self._process_avatar_creation,
            WorkflowStage.RENDERING: self._process_rendering,
        }
        
        logger.info("WorkflowOrchestrator initialized")
    
    def setup_database(self):
        """Setup database per tracking workflows"""
        db_dir = Path(self.workflows_db).parent
        db_dir.mkdir(parents=True, exist_ok=True)
        
        self.conn = sqlite3.connect(self.workflows_db, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        
        cursor = self.conn.cursor()
        
        # Workflows table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                id TEXT PRIMARY KEY,
                config_json TEXT NOT NULL,
                status TEXT NOT NULL,
                current_stage TEXT NOT NULL,
                progress REAL DEFAULT 0.0,
                started_at TIMESTAMP,
                completed_at TIMESTAMP,
                error_message TEXT,
                metadata_json TEXT,
                checkpoints_json TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Workflow logs table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS workflow_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                workflow_id TEXT NOT NULL,
                stage TEXT NOT NULL,
                level TEXT NOT NULL,
                message TEXT NOT NULL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (workflow_id) REFERENCES workflows(id)
            )
        """)
        
        self.conn.commit()
        logger.info("Workflow database initialized")
    
    async def start_workflow(self, config: WorkflowConfig) -> str:
        """
        Avvia nuovo workflow
        
        Args:
            config: Configurazione workflow
            
        Returns:
            ID workflow creato
        """
        workflow_id = str(uuid.uuid4())
        
        # Crea stato workflow
        state = WorkflowState(
            id=workflow_id,
            status=WorkflowStatus.PENDING,
            current_stage=WorkflowStage.INITIALIZED,
            progress=0.0
        )
        
        # Salva nel database
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO workflows (id, config_json, status, current_stage, progress, metadata_json, checkpoints_json)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            workflow_id,
            json.dumps(asdict(config)),
            state.status.value,
            state.current_stage.value,
            state.progress,
            json.dumps(state.metadata),
            json.dumps(state.checkpoints)
        ))
        self.conn.commit()
        
        # Avvia workflow in background
        asyncio.create_task(self._execute_workflow(workflow_id, config))
        
        logger.info(f"Workflow started: {workflow_id}")
        return workflow_id
    
    async def _execute_workflow(self, workflow_id: str, config: WorkflowConfig):
        """
        Esegue workflow completo
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
        """
        try:
            # Aggiorna status
            await self._update_workflow_status(workflow_id, WorkflowStatus.RUNNING, WorkflowStage.INITIALIZED)
            
            # Log workflow start
            await self._log_workflow(workflow_id, WorkflowStage.INITIALIZED, "info", "Workflow started")
            
            # Esegui stages abilitati
            for stage in config.stages_enabled:
                if stage == WorkflowStage.INITIALIZED or stage == WorkflowStage.COMPLETED:
                    continue
                
                # Aggiorna stage corrente
                await self._update_workflow_status(workflow_id, WorkflowStatus.RUNNING, stage)
                
                # Esegui stage
                result = await self._execute_stage(workflow_id, stage, config)
                
                if not result.success:
                    # Gestisci errore
                    await self._handle_stage_error(workflow_id, stage, result.error_message, config)
                    return
                
                # Salva checkpoint
                await self._save_checkpoint(workflow_id, stage, result)
                
                # Aggiorna progress
                progress = (list(config.stages_enabled).index(stage) + 1) / len(config.stages_enabled)
                await self._update_workflow_progress(workflow_id, progress)
            
            # Workflow completato
            await self._update_workflow_status(workflow_id, WorkflowStatus.COMPLETED, WorkflowStage.COMPLETED)
            await self._log_workflow(workflow_id, WorkflowStage.COMPLETED, "info", "Workflow completed successfully")
            
        except Exception as e:
            logger.error(f"Workflow execution error: {e}")
            await self._update_workflow_status(workflow_id, WorkflowStatus.FAILED, WorkflowStage.FAILED, str(e))
            await self._log_workflow(workflow_id, WorkflowStage.FAILED, "error", f"Workflow failed: {str(e)}")
    
    async def _execute_stage(self, workflow_id: str, stage: WorkflowStage, config: WorkflowConfig) -> ProcessingResult:
        """
        Esegue singolo stage
        
        Args:
            workflow_id: ID workflow
            stage: Stage da eseguire
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            start_time = datetime.now()
            
            # Log stage start
            await self._log_workflow(workflow_id, stage, "info", f"Starting stage: {stage.value}")
            
            # Esegui stage processor
            processor = self.stage_processors.get(stage)
            if not processor:
                return ProcessingResult(
                    success=False,
                    error_message=f"No processor for stage: {stage.value}"
                )
            
            result = await processor(workflow_id, config)
            
            # Calcola tempo elaborazione
            processing_time = (datetime.now() - start_time).total_seconds()
            result.processing_time = processing_time
            
            # Log stage completion
            await self._log_workflow(workflow_id, stage, "info", 
                                   f"Stage completed in {processing_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Stage execution error: {e}")
            await self._log_workflow(workflow_id, stage, "error", f"Stage failed: {str(e)}")
            
            return ProcessingResult(
                success=False,
                error_message=str(e)
            )
    
    async def _process_acquisition(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo acquisizione video
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            input_path = Path(config.input_path)
            if not input_path.exists():
                return ProcessingResult(
                    success=False,
                    error_message=f"Input file not found: {input_path}"
                )
            
            # Copia file in temp directory
            temp_input = self.temp_dir / f"{workflow_id}_input{input_path.suffix}"
            shutil.copy2(input_path, temp_input)
            
            # Analizza video
            cap = cv2.VideoCapture(str(temp_input))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0
            cap.release()
            
            metadata = {
                'input_path': str(temp_input),
                'fps': fps,
                'frame_count': frame_count,
                'width': width,
                'height': height,
                'duration': duration,
                'format': input_path.suffix
            }
            
            return ProcessingResult(
                success=True,
                metadata=metadata,
                quality_score=1.0
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Acquisition error: {str(e)}"
            )
    
    async def _process_extraction(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo estrazione pose
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            # Recupera checkpoint precedente
            checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.ACQUISITION)
            if not checkpoint:
                return ProcessingResult(
                    success=False,
                    error_message="No acquisition checkpoint found"
                )
            
            input_path = checkpoint['metadata']['input_path']
            
            # Inizializza MediaPipe
            if not self.pose_detector:
                self.pose_detector = mp.solutions.pose.Pose(
                    static_image_mode=False,
                    model_complexity=2,
                    enable_segmentation=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
            
            # Estrai pose da video
            cap = cv2.VideoCapture(input_path)
            poses_data = []
            frame_idx = 0
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Converti frame per MediaPipe
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose_detector.process(rgb_frame)
                
                if results.pose_landmarks:
                    # Estrai landmarks
                    landmarks = []
                    for landmark in results.pose_landmarks.landmark:
                        landmarks.append({
                            'x': landmark.x,
                            'y': landmark.y,
                            'z': landmark.z,
                            'visibility': landmark.visibility
                        })
                    
                    poses_data.append({
                        'frame_idx': frame_idx,
                        'landmarks': landmarks,
                        'timestamp': frame_idx / checkpoint['metadata']['fps']
                    })
                
                frame_idx += 1
                
                # Checkpoint ogni 100 frame
                if frame_idx % 100 == 0:
                    await self._log_workflow(workflow_id, WorkflowStage.EXTRACTION, 
                                           "info", f"Processed {frame_idx} frames")
            
            cap.release()
            
            # Salva poses data
            poses_file = self.temp_dir / f"{workflow_id}_poses.json"
            with open(poses_file, 'w') as f:
                json.dump(poses_data, f, indent=2)
            
            metadata = {
                'poses_file': str(poses_file),
                'poses_count': len(poses_data),
                'frames_processed': frame_idx,
                'extraction_quality': len(poses_data) / frame_idx if frame_idx > 0 else 0
            }
            
            return ProcessingResult(
                success=True,
                metadata=metadata,
                quality_score=metadata['extraction_quality']
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Extraction error: {str(e)}"
            )
    
    async def _process_fusion(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo fusione skeleton
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            # Recupera checkpoint precedente
            checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.EXTRACTION)
            if not checkpoint:
                return ProcessingResult(
                    success=False,
                    error_message="No extraction checkpoint found"
                )
            
            poses_file = checkpoint['metadata']['poses_file']
            
            # Carica poses data
            with open(poses_file, 'r') as f:
                poses_data = json.load(f)
            
            # Fusione skeleton (algoritmo semplificato)
            fused_skeleton = []
            for i, pose in enumerate(poses_data):
                # Applica smoothing e correzioni
                smoothed_landmarks = []
                for landmark in pose['landmarks']:
                    # Smoothing semplice
                    smoothed_landmark = {
                        'x': landmark['x'],
                        'y': landmark['y'],
                        'z': landmark['z'],
                        'visibility': landmark['visibility'],
                        'confidence': landmark['visibility']
                    }
                    smoothed_landmarks.append(smoothed_landmark)
                
                fused_skeleton.append({
                    'frame_idx': pose['frame_idx'],
                    'timestamp': pose['timestamp'],
                    'landmarks': smoothed_landmarks,
                    'quality_score': sum(l['visibility'] for l in smoothed_landmarks) / len(smoothed_landmarks)
                })
            
            # Salva fused skeleton
            skeleton_file = self.temp_dir / f"{workflow_id}_skeleton.json"
            with open(skeleton_file, 'w') as f:
                json.dump(fused_skeleton, f, indent=2)
            
            metadata = {
                'skeleton_file': str(skeleton_file),
                'skeleton_count': len(fused_skeleton),
                'average_quality': sum(s['quality_score'] for s in fused_skeleton) / len(fused_skeleton)
            }
            
            return ProcessingResult(
                success=True,
                metadata=metadata,
                quality_score=metadata['average_quality']
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Fusion error: {str(e)}"
            )
    
    async def _process_translation(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo traduzione audio
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            # Recupera checkpoint precedente
            checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.FUSION)
            if not checkpoint:
                return ProcessingResult(
                    success=False,
                    error_message="No fusion checkpoint found"
                )
            
            # Inizializza translator se non presente
            if not self.translator:
                try:
                    self.translator = pipeline("translation", 
                                             model="Helsinki-NLP/opus-mt-en-it",
                                             device=0 if torch.cuda.is_available() else -1)
                except Exception as e:
                    logger.warning(f"Translation model not available: {e}")
                    self.translator = None
            
            # Voice cloning e traduzione REALE
            from voice_cloning import VoiceCloningService, VoiceLanguage
            
            # Inizializza voice cloning service
            voice_config = {
                'models_dir': str(self.temp_dir / 'voice_models'),
                'temp_dir': str(self.temp_dir)
            }
            voice_service = VoiceCloningService(voice_config)
            
            # Ottieni file video
            video_path = checkpoint['data']['output_video']
            
            # Clona voce dal video
            synthesis_result = await voice_service.clone_voice_from_video(
                video_path=video_path,
                text="Sample text for voice cloning",
                start_time=0,
                duration=10
            )
            
            # Traduzione con voice synthesis
            translated_result = await voice_service.translate_and_synthesize(
                text="Sample text for translation",
                source_lang=VoiceLanguage.ENGLISH,
                target_lang=VoiceLanguage.ITALIAN,
                voice_model=voice_service.get_available_models()[0] if voice_service.get_available_models() else None
            )
            
            translation_result = {
                'source_language': 'en',
                'target_language': 'it',
                'translation_quality': synthesis_result.quality_score,
                'translated_segments': [],
                'voice_cloned_audio': synthesis_result.audio_path,
                'translated_audio': translated_result.audio_path,
                'voice_cloning_used': True
            }
            
            # Salva translation data
            translation_file = self.temp_dir / f"{workflow_id}_translation.json"
            with open(translation_file, 'w') as f:
                json.dump(translation_result, f, indent=2)
            
            metadata = {
                'translation_file': str(translation_file),
                'source_language': translation_result['source_language'],
                'target_language': translation_result['target_language'],
                'translation_quality': translation_result['translation_quality']
            }
            
            return ProcessingResult(
                success=True,
                metadata=metadata,
                quality_score=translation_result['translation_quality']
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Translation error: {str(e)}"
            )
    
    async def _process_avatar_creation(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo creazione avatar
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            # Recupera checkpoint precedenti
            fusion_checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.FUSION)
            translation_checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.TRANSLATION)
            
            if not fusion_checkpoint or not translation_checkpoint:
                return ProcessingResult(
                    success=False,
                    error_message="Missing required checkpoints for avatar creation"
                )
            
            # Simula creazione avatar
            avatar_result = {
                'avatar_type': 'pose_based',
                'skeleton_file': fusion_checkpoint['metadata']['skeleton_file'],
                'translation_file': translation_checkpoint['metadata']['translation_file'],
                'avatar_quality': 0.90,
                'rendering_ready': True
            }
            
            # Salva avatar data
            avatar_file = self.temp_dir / f"{workflow_id}_avatar.json"
            with open(avatar_file, 'w') as f:
                json.dump(avatar_result, f, indent=2)
            
            metadata = {
                'avatar_file': str(avatar_file),
                'avatar_type': avatar_result['avatar_type'],
                'avatar_quality': avatar_result['avatar_quality'],
                'rendering_ready': avatar_result['rendering_ready']
            }
            
            return ProcessingResult(
                success=True,
                metadata=metadata,
                quality_score=avatar_result['avatar_quality']
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Avatar creation error: {str(e)}"
            )
    
    async def _process_rendering(self, workflow_id: str, config: WorkflowConfig) -> ProcessingResult:
        """
        Processo rendering finale
        
        Args:
            workflow_id: ID workflow
            config: Configurazione workflow
            
        Returns:
            Risultato elaborazione
        """
        try:
            # Recupera checkpoint precedente
            checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.AVATAR_CREATION)
            if not checkpoint:
                return ProcessingResult(
                    success=False,
                    error_message="No avatar creation checkpoint found"
                )
            
            # Genera output path
            output_filename = f"{workflow_id}_processed.mp4"
            output_path = self.output_dir / output_filename
            
            # Simula rendering (per ora copia input)
            acquisition_checkpoint = await self._get_checkpoint(workflow_id, WorkflowStage.ACQUISITION)
            if acquisition_checkpoint:
                input_path = acquisition_checkpoint['metadata']['input_path']
                shutil.copy2(input_path, output_path)
            
            metadata = {
                'output_path': str(output_path),
                'output_format': 'mp4',
                'rendering_time': 30.0,  # Simulated
                'final_quality': 0.95
            }
            
            return ProcessingResult(
                success=True,
                output_path=str(output_path),
                metadata=metadata,
                quality_score=metadata['final_quality']
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error_message=f"Rendering error: {str(e)}"
            )
    
    async def _handle_stage_error(self, workflow_id: str, stage: WorkflowStage, 
                                error_message: str, config: WorkflowConfig):
        """
        Gestisce errore in stage
        
        Args:
            workflow_id: ID workflow
            stage: Stage fallito
            error_message: Messaggio errore
            config: Configurazione workflow
        """
        # Log errore
        await self._log_workflow(workflow_id, stage, "error", f"Stage failed: {error_message}")
        
        # Aggiorna status workflow
        await self._update_workflow_status(workflow_id, WorkflowStatus.FAILED, WorkflowStage.FAILED, error_message)
    
    async def _save_checkpoint(self, workflow_id: str, stage: WorkflowStage, result: ProcessingResult):
        """
        Salva checkpoint per stage
        
        Args:
            workflow_id: ID workflow
            stage: Stage completato
            result: Risultato elaborazione
        """
        try:
            # Recupera stato workflow
            cursor = self.conn.cursor()
            cursor.execute("SELECT checkpoints_json FROM workflows WHERE id = ?", (workflow_id,))
            row = cursor.fetchone()
            
            if row:
                checkpoints = json.loads(row['checkpoints_json'])
            else:
                checkpoints = {}
            
            # Aggiungi checkpoint
            checkpoints[stage.value] = {
                'timestamp': datetime.now().isoformat(),
                'success': result.success,
                'metadata': result.metadata,
                'quality_score': result.quality_score,
                'processing_time': result.processing_time
            }
            
            # Salva nel database
            cursor.execute("""
                UPDATE workflows SET checkpoints_json = ?, updated_at = ?
                WHERE id = ?
            """, (json.dumps(checkpoints), datetime.now(), workflow_id))
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"Error saving checkpoint: {e}")
    
    async def _get_checkpoint(self, workflow_id: str, stage: WorkflowStage) -> Optional[Dict[str, Any]]:
        """
        Recupera checkpoint per stage
        
        Args:
            workflow_id: ID workflow
            stage: Stage richiesto
            
        Returns:
            Checkpoint data o None
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT checkpoints_json FROM workflows WHERE id = ?", (workflow_id,))
            row = cursor.fetchone()
            
            if row:
                checkpoints = json.loads(row['checkpoints_json'])
                return checkpoints.get(stage.value)
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting checkpoint: {e}")
            return None
    
    async def _update_workflow_status(self, workflow_id: str, status: WorkflowStatus, 
                                    current_stage: WorkflowStage, error_message: Optional[str] = None):
        """
        Aggiorna status workflow
        
        Args:
            workflow_id: ID workflow
            status: Nuovo status
            current_stage: Stage corrente
            error_message: Messaggio errore se presente
        """
        try:
            cursor = self.conn.cursor()
            
            update_fields = {
                'status': status.value,
                'current_stage': current_stage.value,
                'updated_at': datetime.now()
            }
            
            if error_message:
                update_fields['error_message'] = error_message
            
            if status == WorkflowStatus.RUNNING and current_stage == WorkflowStage.INITIALIZED:
                update_fields['started_at'] = datetime.now()
            elif status in [WorkflowStatus.COMPLETED, WorkflowStatus.FAILED, WorkflowStatus.CANCELLED]:
                update_fields['completed_at'] = datetime.now()
            
            # Build update query
            set_clause = ', '.join([f"{k} = ?" for k in update_fields.keys()])
            values = list(update_fields.values()) + [workflow_id]
            
            cursor.execute(f"""
                UPDATE workflows SET {set_clause}
                WHERE id = ?
            """, values)
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"Error updating workflow status: {e}")
    
    async def _update_workflow_progress(self, workflow_id: str, progress: float):
        """
        Aggiorna progress workflow
        
        Args:
            workflow_id: ID workflow
            progress: Progress (0.0 - 1.0)
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                UPDATE workflows SET progress = ?, updated_at = ?
                WHERE id = ?
            """, (progress, datetime.now(), workflow_id))
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"Error updating workflow progress: {e}")
    
    async def _log_workflow(self, workflow_id: str, stage: WorkflowStage, 
                          level: str, message: str):
        """
        Log workflow event
        
        Args:
            workflow_id: ID workflow
            stage: Stage corrente
            level: Livello log
            message: Messaggio
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                INSERT INTO workflow_logs (workflow_id, stage, level, message)
                VALUES (?, ?, ?, ?)
            """, (workflow_id, stage.value, level, message))
            
            self.conn.commit()
            
            # Log anche su logger
            log_func = getattr(logger, level, logger.info)
            log_func(f"Workflow {workflow_id} [{stage.value}]: {message}")
            
        except Exception as e:
            logger.error(f"Error logging workflow event: {e}")
    
    async def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """
        Ottiene status workflow
        
        Args:
            workflow_id: ID workflow
            
        Returns:
            Status workflow o None
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM workflows WHERE id = ?
            """, (workflow_id,))
            
            row = cursor.fetchone()
            if row:
                return {
                    'id': row['id'],
                    'status': row['status'],
                    'current_stage': row['current_stage'],
                    'progress': row['progress'],
                    'started_at': row['started_at'],
                    'completed_at': row['completed_at'],
                    'error_message': row['error_message'],
                    'metadata': json.loads(row['metadata_json']) if row['metadata_json'] else {},
                    'checkpoints': json.loads(row['checkpoints_json']) if row['checkpoints_json'] else {}
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting workflow status: {e}")
            return None
    
    async def cancel_workflow(self, workflow_id: str) -> bool:
        """
        Cancella workflow
        
        Args:
            workflow_id: ID workflow
            
        Returns:
            True se cancellato con successo
        """
        try:
            await self._update_workflow_status(workflow_id, WorkflowStatus.CANCELLED, WorkflowStage.FAILED)
            await self._log_workflow(workflow_id, WorkflowStage.FAILED, "info", "Workflow cancelled by user")
            
            return True
            
        except Exception as e:
            logger.error(f"Error cancelling workflow: {e}")
            return False
    
    async def cleanup_workflow(self, workflow_id: str):
        """
        Cleanup risorse workflow
        
        Args:
            workflow_id: ID workflow
        """
        try:
            # Rimuovi file temporanei
            temp_files = list(self.temp_dir.glob(f"{workflow_id}_*"))
            for temp_file in temp_files:
                try:
                    temp_file.unlink()
                except Exception as e:
                    logger.warning(f"Could not remove temp file {temp_file}: {e}")
            
            logger.info(f"Cleanup completed for workflow: {workflow_id}")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
    
    def close(self):
        """Chiude connessioni e cleanup"""
        if hasattr(self, 'conn'):
            self.conn.close()
        
        if self.pose_detector:
            self.pose_detector.close()
        
        logger.info("WorkflowOrchestrator closed")

