"""
ðŸŽ“ AI_MODULE: Anonymizer Service (Privacy by Design)
ðŸŽ“ AI_DESCRIPTION: Servizio per anonimizzazione dati pre-mix con FORGETTING BY DESIGN
ðŸŽ“ AI_BUSINESS: Garantisce che NESSUNA fonte sia tracciabile nel mix finale - compliance GDPR
ðŸŽ“ AI_TEACHING: Text processing, paraphrasing, content aggregation, privacy patterns

ðŸ”„ ALTERNATIVE_VALUTATE:
- Simple hash/rename: Scartato, metadata possono rivelare fonte
- Encryption: Scartato, file criptati ancora identificabili
- Differential privacy: Troppo complesso per questo caso
- Manual review: Non scalabile

ðŸ’¡ PERCHÃ‰_QUESTA_SOLUZIONE:
- Privacy: Parafrasatura automatica impedisce quote dirette
- Aggregazione: Mescola contenuti da 3+ fonti quando possibile
- Randomizzazione: Ordine casuale rompe sequenza originale
- Completezza: Rimuove metadata, nomi file, riferimenti

ðŸ“Š METRICHE_SUCCESSO:
- Privacy leak rate: 0%
- Paraphrase accuracy: > 95%
- Processing speed: < 100ms per item
- Aggregation ratio: > 70% items aggregati

ðŸ”— INTEGRATION_DEPENDENCIES:
- Upstream: ProjectManager, LLM Debate (per parafrasatura)
- Downstream: MixGenerator
"""

import re
import random
import hashlib
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


# === AI SIGNATURE PATTERNS ===
# Pattern per rimuovere firme di altre AI

AI_SIGNATURE_PATTERNS = [
    # OpenAI/ChatGPT patterns
    r"As an AI( language model)?[,.].*?(?=\n|$)",
    r"I('m| am) an AI( assistant)?[,.].*?(?=\n|$)",
    r"As a large language model[,.].*?(?=\n|$)",
    r"I don't have personal (opinions|experiences|feelings).*?(?=\n|$)",
    r"I cannot (browse|access|see) (the internet|real-time|images).*?(?=\n|$)",
    r"My knowledge (cutoff|was last updated).*?(?=\n|$)",
    r"Generated by (ChatGPT|GPT-\d|OpenAI).*?(?=\n|$)",

    # Anthropic/Claude patterns
    r"I'm Claude[,.].*?(?=\n|$)",
    r"I was created by Anthropic.*?(?=\n|$)",
    r"As Claude[,.].*?(?=\n|$)",

    # Generic AI patterns
    r"AI-generated.*?(?=\n|$)",
    r"\[AI Response\].*?(?=\n|$)",
    r"\[Generated by AI\].*?(?=\n|$)",
]

# Pattern per riferimenti a fonti
SOURCE_REFERENCE_PATTERNS = [
    r"(?:according to|from|source:|based on|as stated in)\s*['\"]?[\w\s\-\.]+['\"]?",
    r"(?:the book|the video|the document)\s*['\"][\w\s\-\.]+['\"]",
    r"(?:page|p\.)\s*\d+",
    r"(?:chapter|ch\.)\s*\d+",
    r"(?:at timestamp|at \d+:\d+)",
    r"(?:file|documento):\s*[\w\-\.]+",
    r"(?:https?://|www\.)[^\s]+",  # URLs
]

# Parole chiave di fonti da rimuovere
FORBIDDEN_SOURCE_WORDS = {
    "source", "from", "according", "based on", "reference",
    "libro", "video", "documento", "file", "page", "pagina",
    "capitolo", "chapter", "timestamp", "url", "link"
}


class Anonymizer:
    """
    ðŸ”’ Servizio per anonimizzazione contenuti.

    PRINCIPIO: FORGETTING BY DESIGN
    - Nessun nome file originale
    - Nessun riferimento a fonti
    - Parafrasatura obbligatoria
    - Aggregazione dove possibile
    - Ordine randomizzato
    """

    def __init__(self, llm_service=None):
        """
        Inizializza l'anonymizer.

        Args:
            llm_service: Servizio LLM per parafrasatura (opzionale)
        """
        self.llm_service = llm_service
        self._seen_hashes: Set[str] = set()

    # =========================================================================
    # MAIN ANONYMIZATION
    # =========================================================================

    def anonymize_for_mix(self, extracted_data: List[Dict[str, Any]]) -> Dict[str, List[Dict]]:
        """
        ðŸ”’ ANONIMIZZA tutto prima del MIX.

        REGOLE FERREE:
        1. Nessun nome file originale
        2. Nessun riferimento a libri/maestri/fonti
        3. Parafrasa tutto (mai copia letterale)
        4. Mescola ordine (non sequenza originale)
        5. Aggrega da 3+ fonti quando possibile
        6. No metadata origine

        Args:
            extracted_data: Lista di dati estratti con metadata

        Returns:
            Dict con knowledge, vocabulary, skeleton, techniques (anonimizzati)
        """
        logger.info(f"ðŸ”’ Anonimizzazione {len(extracted_data)} items")

        anonymized = {
            "knowledge": [],
            "vocabulary": [],
            "skeleton": [],
            "techniques": [],
            # ðŸ”’ MAI: "sources", "files", "origins", "references"
        }

        for item in extracted_data:
            try:
                clean = self._anonymize_item(item)
                if clean:
                    item_type = item.get("type", "knowledge")
                    if item_type in anonymized:
                        anonymized[item_type].append(clean)
                    else:
                        anonymized["knowledge"].append(clean)
            except Exception as e:
                logger.warning(f"âš ï¸ Errore anonimizzazione item: {e}")

        # ðŸ”’ SHUFFLE: Mescola ordine per rompere sequenza originale
        for key in anonymized:
            random.shuffle(anonymized[key])

        # ðŸ”’ AGGREGATE: Aggrega items simili (3+ â†’ 1)
        anonymized["knowledge"] = self._aggregate_similar(anonymized["knowledge"])

        logger.info(f"âœ… Anonimizzazione completata: {sum(len(v) for v in anonymized.values())} items")

        return anonymized

    def _anonymize_item(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Anonimizza un singolo item.

        ðŸ”’ PRIVACY:
        - Rimuove tutti i campi identificativi
        - Parafrasa il contenuto
        - Rimuove riferimenti a fonti
        """
        content = item.get("content", "")
        if not content:
            return None

        # 1. Rimuovi riferimenti a fonti nel contenuto
        clean_content = self._remove_source_references(content)

        # 2. Parafrasa (se LLM disponibile) o pulisci
        if self.llm_service:
            clean_content = self._paraphrase_content(clean_content)
        else:
            clean_content = self._basic_cleanup(clean_content)

        # 3. Crea item pulito (ðŸ”’ solo campi sicuri)
        clean_item = {
            "content": clean_content,
            "type": item.get("type", "knowledge"),
            "lang": item.get("lang", "it"),
            # ðŸ”’ MAI includere:
            # - "source_file"
            # - "page"
            # - "timestamp"
            # - "author"
            # - "book"
            # - "original_filename"
        }

        # Aggiungi hash per dedup (non identificativo)
        content_hash = hashlib.sha256(clean_content.encode()).hexdigest()[:16]
        if content_hash in self._seen_hashes:
            return None  # Skip duplicato
        self._seen_hashes.add(content_hash)

        return clean_item

    # =========================================================================
    # TEXT CLEANING
    # =========================================================================

    def remove_ai_signatures(self, text: str) -> str:
        """
        ðŸ”’ Rimuove firme di altre AI (ChatGPT, Claude, etc.).

        Usato per paste da altre AI.
        """
        result = text

        for pattern in AI_SIGNATURE_PATTERNS:
            result = re.sub(pattern, "", result, flags=re.IGNORECASE | re.MULTILINE)

        # Rimuovi righe vuote multiple
        result = re.sub(r'\n{3,}', '\n\n', result)

        return result.strip()

    def _remove_source_references(self, text: str) -> str:
        """
        ðŸ”’ Rimuove tutti i riferimenti a fonti dal testo.
        """
        result = text

        for pattern in SOURCE_REFERENCE_PATTERNS:
            result = re.sub(pattern, "", result, flags=re.IGNORECASE)

        # Rimuovi parole chiave isolate
        words = result.split()
        words = [w for w in words if w.lower() not in FORBIDDEN_SOURCE_WORDS]
        result = " ".join(words)

        # Pulisci punteggiatura orfana
        result = re.sub(r'\s+([,.:;])', r'\1', result)
        result = re.sub(r'\s{2,}', ' ', result)

        return result.strip()

    def _basic_cleanup(self, text: str) -> str:
        """
        Pulizia base del testo (senza LLM).
        """
        # Normalizza whitespace
        text = re.sub(r'\s+', ' ', text)

        # Rimuovi caratteri di controllo
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)

        # Normalizza quote
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(''', "'").replace(''', "'")

        return text.strip()

    def _paraphrase_content(self, text: str) -> str:
        """
        ðŸ”’ Parafrasa il contenuto usando LLM.

        MAI copia letterale - sempre riformulazione.
        """
        if not self.llm_service:
            return self._basic_cleanup(text)

        # System prompt per anonimizzazione
        system_prompt = """
You are paraphrasing martial arts content for anonymization.

ABSOLUTE RULES:
- NEVER keep the original phrasing - always rephrase completely
- NEVER mention source names, book titles, video names, author names
- NEVER say "according to [source]" or "from [source]"
- NEVER include metadata about where information came from
- Output ONLY the knowledge itself, not its origin
- Keep the same meaning but change the words
- Keep technical terms (technique names in original language)
"""

        try:
            # Chiamata LLM per parafrasatura
            paraphrased = self.llm_service.generate(
                prompt=f"Paraphrase this martial arts content:\n\n{text}",
                system=system_prompt,
                max_tokens=len(text) * 2,
                temperature=0.7  # Un po' di creativitÃ  per variare
            )
            return paraphrased.strip()
        except Exception as e:
            logger.warning(f"âš ï¸ Parafrasatura fallita, uso cleanup base: {e}")
            return self._basic_cleanup(text)

    # =========================================================================
    # AGGREGATION
    # =========================================================================

    def _aggregate_similar(self, items: List[Dict]) -> List[Dict]:
        """
        ðŸ”’ Aggrega items simili per aumentare anonimizzazione.

        Quando possibile, combina 3+ items simili in 1.
        """
        if len(items) < 3:
            return items

        # Raggruppa per tipo contenuto
        groups: Dict[str, List[Dict]] = {}
        for item in items:
            item_type = item.get("type", "general")
            if item_type not in groups:
                groups[item_type] = []
            groups[item_type].append(item)

        result = []
        for group_type, group_items in groups.items():
            if len(group_items) >= 3:
                # Aggrega in gruppi di 3
                for i in range(0, len(group_items), 3):
                    chunk = group_items[i:i+3]
                    if len(chunk) >= 2:
                        aggregated = self._merge_items(chunk)
                        result.append(aggregated)
                    else:
                        result.extend(chunk)
            else:
                result.extend(group_items)

        return result

    def _merge_items(self, items: List[Dict]) -> Dict:
        """
        Unisce piÃ¹ items in uno aggregato.
        """
        contents = [item.get("content", "") for item in items]
        merged_content = " ".join(contents)

        # Determina lingua predominante
        langs = [item.get("lang", "it") for item in items]
        main_lang = max(set(langs), key=langs.count)

        return {
            "content": merged_content,
            "type": items[0].get("type", "knowledge"),
            "lang": main_lang,
            "aggregated_count": len(items),
            # ðŸ”’ MAI: "source_items", "original_files"
        }

    # =========================================================================
    # VALIDATION
    # =========================================================================

    def validate_anonymized(self, data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        ðŸ”’ Valida che i dati siano correttamente anonimizzati.

        Controlla che non ci siano leak di informazioni.

        Returns:
            Dict con risultati validazione
        """
        violations = []
        checked_items = 0

        for category, items in data.items():
            for item in items:
                checked_items += 1

                # Check campi vietati
                forbidden_fields = ["source", "source_file", "original_filename",
                                    "page", "timestamp", "author", "book", "url"]
                for field in forbidden_fields:
                    if field in item:
                        violations.append(f"Campo vietato '{field}' in {category}")

                # Check contenuto per pattern vietati
                content = str(item.get("content", ""))
                for pattern in SOURCE_REFERENCE_PATTERNS:
                    if re.search(pattern, content, re.IGNORECASE):
                        violations.append(f"Pattern fonte trovato in {category}: {pattern}")

        return {
            "is_valid": len(violations) == 0,
            "items_checked": checked_items,
            "violations": violations,
            "checked_at": datetime.utcnow().isoformat()
        }

    # =========================================================================
    # UTILITY
    # =========================================================================

    def anonymize_filename(self, original: str) -> str:
        """
        ðŸ”’ Genera filename anonimo da originale.

        NON preserva nessuna informazione dal nome originale.
        """
        import uuid
        from pathlib import Path

        # Prendi solo estensione
        ext = Path(original).suffix if original else ""

        # UUID completamente nuovo
        return f"{uuid.uuid4()}{ext}"

    def anonymize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ”’ Rimuove metadata identificativi.
        """
        # Campi permessi (non identificativi)
        allowed = {"duration", "width", "height", "fps", "lang",
                   "type", "confidence", "word_count"}

        return {k: v for k, v in metadata.items() if k in allowed}

    def clear_cache(self):
        """Reset cache hash per nuovo batch."""
        self._seen_hashes.clear()


# === FACTORY FUNCTION ===

def create_anonymizer(llm_service=None) -> Anonymizer:
    """
    Factory per creare Anonymizer.

    Args:
        llm_service: Servizio LLM opzionale per parafrasatura avanzata
    """
    return Anonymizer(llm_service=llm_service)
