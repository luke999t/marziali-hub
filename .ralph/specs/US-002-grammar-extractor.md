# US-002: Grammar Extractor - Asian Languages

## Overview
Extract grammar rules from Chinese/Japanese/Korean text for martial arts learning.

## Technical Requirements

### 1. Grammar Extractor (`backend/services/language_learning/grammar_extractor.py`)

```python
"""
ğŸ“ AI_MODULE: GrammarExtractor
ğŸ“ AI_DESCRIPTION: Extracts grammar patterns from CJK text
ğŸ“ AI_BUSINESS: Enables martial arts terminology learning
ğŸ“ AI_TEACHING: NLP tokenization with language-specific libraries
"""

class GrammarExtractor:
    def __init__(self)
    def extract_chinese(self, text: str) -> GrammarResult
    def extract_japanese(self, text: str) -> GrammarResult
    def extract_korean(self, text: str) -> GrammarResult
    def detect_language(self, text: str) -> str
```

### 2. Chinese Extraction

**Library:** jieba (segmentation) + pypinyin (pronunciation)

**Extract:**
- Word segmentation
- Pinyin pronunciation
- Character components (radicals)
- Common patterns (measure words, particles)

**Tasks:**
- [ ] Install jieba, pypinyin
- [ ] Implement extract_chinese()
- [ ] Return word list with pinyin
- [ ] Identify grammar particles (çš„, äº†, åœ¨, etc.)

### 3. Japanese Extraction

**Library:** fugashi (MeCab wrapper)

**Extract:**
- Morphological analysis
- Verb conjugations (dictionary form, te-form, etc.)
- Particles (ã¯, ãŒ, ã‚’, ã«, etc.)
- Kanji readings

**Tasks:**
- [ ] Install fugashi, unidic-lite
- [ ] Implement extract_japanese()
- [ ] Return word list with readings
- [ ] Identify verb forms and conjugations

### 4. Korean Extraction

**Library:** konlpy (Okt tokenizer)

**Extract:**
- Morpheme analysis
- Verb conjugations
- Particles (ì€/ëŠ”, ì´/ê°€, ì„/ë¥¼)
- Honorific levels

**Tasks:**
- [ ] Install konlpy
- [ ] Implement extract_korean()
- [ ] Return morpheme list
- [ ] Identify grammar particles

### 5. API Endpoint (`backend/api/v1/endpoints/grammar_learning.py`)

```python
@router.post("/grammar/extract")
async def extract_grammar(request: GrammarRequest) -> GrammarResponse:
    """
    POST /api/v1/grammar/extract
    Body: {"text": "å¤ªææ‹³æ˜¯ä¸­å›½æ­¦æœ¯", "language": "zh"}
    Response: {
        "words": [
            {"word": "å¤ªææ‹³", "pinyin": "tÃ i jÃ­ quÃ¡n", "type": "noun"},
            {"word": "æ˜¯", "pinyin": "shÃ¬", "type": "verb"},
            ...
        ],
        "particles": ["æ˜¯"],
        "patterns": ["N + æ˜¯ + N"]
    }
    """
```

**Tasks:**
- [ ] Create router with POST endpoint
- [ ] Auto-detect language if not specified
- [ ] Return structured grammar data
- [ ] Add to main.py router includes

### 6. Unit Tests (`backend/tests/unit/test_grammar_extractor.py`)

```python
"""
âš ï¸ ZERO MOCK TEST - Tests real extraction
No network required, but real libraries must work
"""

def test_chinese_extraction():
    extractor = GrammarExtractor()
    result = extractor.extract_chinese("å¤ªææ‹³æ˜¯ä¸­å›½æ­¦æœ¯")
    assert len(result.words) > 0
    assert "å¤ªææ‹³" in [w.word for w in result.words]

def test_japanese_extraction():
    extractor = GrammarExtractor()
    result = extractor.extract_japanese("ç©ºæ‰‹ã¯æ—¥æœ¬ã®æ­¦é“ã§ã™")
    assert len(result.words) > 0
```

**Tasks:**
- [ ] Test Chinese extraction
- [ ] Test Japanese extraction
- [ ] Test Korean extraction
- [ ] Test language detection
- [ ] Test edge cases (empty, mixed language)

## Dependencies

```txt
# Chinese
jieba>=0.42.1
pypinyin>=0.48.0

# Japanese
fugashi>=1.3.0
unidic-lite>=1.0.8

# Korean
konlpy>=0.6.0
```

## Sample Data

### Chinese Martial Arts Terms
```
å¤ªææ‹³ - Tai Chi
å°‘æ—åŠŸå¤« - Shaolin Kung Fu
å’æ˜¥æ‹³ - Wing Chun
å…«å¦æŒ - Baguazhang
```

### Japanese Martial Arts Terms
```
ç©ºæ‰‹ - Karate
æŸ”é“ - Judo
å‰£é“ - Kendo
åˆæ°—é“ - Aikido
```

### Korean Martial Arts Terms
```
íƒœê¶Œë„ - Taekwondo
í•©ê¸°ë„ - Hapkido
ì”¨ë¦„ - Ssireum
```

## Acceptance Criteria Checklist

- [ ] Extracts particles and verb conjugations
- [ ] Supports Chinese (simplified/traditional)
- [ ] Supports Japanese
- [ ] Supports Korean
- [ ] Stores rules in backend/data/grammar/
- [ ] API endpoint POST /api/v1/grammar/extract works
- [ ] ZERO MOCK test with real text samples

## Estimated Effort

- Chinese extraction: 2 hours
- Japanese extraction: 3 hours (MeCab setup)
- Korean extraction: 2 hours
- API Endpoint: 1 hour
- Tests: 2 hours
- **Total: 10 hours**
